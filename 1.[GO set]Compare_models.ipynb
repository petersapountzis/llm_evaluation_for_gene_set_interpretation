{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model comparison\n",
    "\n",
    "**gpt model**\n",
    "\n",
    "gpt4-1103-preview\n",
    "\n",
    "**local model on server**\n",
    "new available models through api: https://api.llm.ideker.ucsd.edu/api/chat\n",
    "\n",
    "available models:\n",
    "\n",
    "| NAME           | ID           | SIZE   |\n",
    "|----------------|--------------|--------|\n",
    "| llama2:70b     | c3a7af098300 | 38 GB  |\n",
    "| llama2:7b      | fe938a131f40 | 3.8 GB |\n",
    "| llama2:latest  | fe938a131f40 | 3.8 GB |\n",
    "| mistral:7b     | 4d9f4b269c33 | 4.1 GB |\n",
    "| mixtral:latest | 99a9202f8a7a | 26 GB  |\n",
    "\n",
    "\n",
    "**API for calling Google Gemini pro**\n",
    "\n",
    "GO TO: https://makersuite.google.com/app/apikey to get the apikey for gemini pro\n",
    "\n",
    "export GOOGLEAI_KEY = xxxx\n",
    "\n",
    "model = 'gemini-pro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "from utils.openai_query import openai_chat\n",
    "from utils.prompt_factory import make_user_prompt_with_score\n",
    "from utils.server_model_query import server_model_chat\n",
    "from utils.llm_analysis_utils import process_analysis, save_progress\n",
    "from utils.genai_query import query_genai_model\n",
    "from tqdm import tqdm\n",
    "import constant\n",
    "import openai\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default run is using GPT4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load variables\n",
    "initialize = True # if True, then initialize the input table with llm names, analysis and score to None \n",
    "# Replace with your actual values\n",
    "config_file = './jsonFiles/model_comparison_gpt4.json'  # replace with your actual config file \n",
    "input_file = 'data/GO_term_analysis/model_comparison_terms.csv' # replace with your actual input file\n",
    "input_sep = ','  # replace with the separator\n",
    "set_index = 'GO'  # replace with your column name that you want to set as index or None\n",
    "gene_column = 'Genes'  # replace with your actual column name for the gene list\n",
    "gene_sep = ' '  # replace with your actual separator\n",
    "gene_features = None  # replace with your path to the gene features or None if you don't want to include in the prompt\n",
    "direct = False # if True, then the prompt will be a direct sentence asking for a name and analysis from the gene set, otherwise default or customized prompt\n",
    "out_file = 'data/GO_term_analysis/trial'  # replace with your actual output file name\n",
    "\n",
    "customized_prompt = False # if True, then the prompt will be the custom prompt, if False, then the prompt will use default\n",
    "\n",
    "# load the config file\n",
    "with open(config_file) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "if customized_prompt:\n",
    "    # make sure the file exist \n",
    "    if os.path.isfile(config['CUSTOM_PROMPT_FILE']):\n",
    "        with open(config['CUSTOM_PROMPT_FILE'], 'r') as f: # replace with your actual customized prompt file\n",
    "            customized_prompt = f.read()\n",
    "            assert len(customized_prompt) > 1, \"Customized prompt is empty\"\n",
    "    else:\n",
    "        print(\"Customized prompt file does not exist\")\n",
    "        customized_prompt = None\n",
    "else:\n",
    "    customized_prompt = None\n",
    "\n",
    "# Load OpenAI key, context, and model used \n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "context = config['CONTEXT']\n",
    "model = config['MODEL']\n",
    "temperature = config['TEMP']\n",
    "max_tokens = config['MAX_TOKENS']\n",
    "if model.startswith('gpt'):\n",
    "    rate_per_token = config['RATE_PER_TOKEN']\n",
    "    DOLLAR_LIMIT = config['DOLLAR_LIMIT']\n",
    "LOG_FILE = config['LOG_NAME']+'_log.json'\n",
    "\n",
    "SEED = constant.SEED\n",
    "column_prefix = model.split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the logger so it create a new one for each model run\n",
    "def get_logger(filename):\n",
    "    logger = logging.getLogger(filename)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(filename)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def main(df):\n",
    "    analysis_dict  = {}\n",
    "\n",
    "    logger = get_logger(f'{out_file}.log')\n",
    "\n",
    "    i = 0 #used for track progress and saving the file\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        #only process None rows \n",
    "        if pd.notna(row[f'{column_prefix} Analysis']):\n",
    "            continue\n",
    "        \n",
    "        gene_data = row[gene_column]\n",
    "        # if gene_data is not a string, then skip\n",
    "        if type(gene_data) != str:\n",
    "            \n",
    "            logger.warning(f'Gene set {idx} is not a string, skipping')\n",
    "            continue\n",
    "        genes = gene_data.split(gene_sep)\n",
    "        \n",
    "        if len(genes) >1000:\n",
    "            logger.warning(f'Gene set {idx} is too big, skipping')\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            prompt = make_user_prompt_with_score(genes)\n",
    "            # print(prompt)\n",
    "            finger_print = None\n",
    "            if model.startswith('gpt'):\n",
    "                print(\"Accessing OpenAI API\")\n",
    "                analysis, finger_print = openai_chat(context, prompt, model, temperature, max_tokens, rate_per_token, LOG_FILE, DOLLAR_LIMIT, SEED)\n",
    "            elif model.startswith('gemini'):\n",
    "                print(\"Using Google Gemini API\")\n",
    "                analysis, error_message = query_genai_model(f\"{context}\\n{prompt}\", model, temperature, max_tokens, LOG_FILE) \n",
    "            else:\n",
    "                print(\"Using server model\")\n",
    "                analysis, error_message= server_model_chat(context, prompt, model, temperature, max_tokens,LOG_FILE, SEED)\n",
    "\n",
    "            \n",
    "            if analysis:\n",
    "                # print(analysis)\n",
    "                llm_name, llm_score, llm_analysis = process_analysis(analysis)\n",
    "                # clean up the score and return float\n",
    "                try:\n",
    "                    llm_score_value =  float(re.sub(\"[^0-9.-]\", \"\", llm_score))\n",
    "                except ValueError:\n",
    "                    llm_score_value = llm_score\n",
    "            \n",
    "                \n",
    "                df.loc[idx, f'{column_prefix} Name'] = llm_name\n",
    "                df.loc[idx, f'{column_prefix} Analysis'] = llm_analysis\n",
    "                df.loc[idx, f'{column_prefix} Score'] = llm_score_value\n",
    "                analysis_dict[f'{idx}_{column_prefix}'] = analysis\n",
    "                # Log success with fingerprint\n",
    "                logger.info(f'Success for {idx} {column_prefix}.')\n",
    "                if finger_print:\n",
    "                    logger.info(f'GPT_Fingerprint for {idx}: {finger_print}')\n",
    "                    \n",
    "            else:\n",
    "                logger.error(f'Error for query gene set {idx}: {error_message}')\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error for {idx}: {e}')\n",
    "            continue\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            save_progress(df, analysis_dict, out_file)\n",
    "            # df.to_csv(f'{out_file}.tsv', sep='\\t', index=True)\n",
    "            print(f\"Saved progress for {i} genesets\")\n",
    "    # save the final file\n",
    "    save_progress(df, analysis_dict, out_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--config ./jsonFiles/toyexample_mixtral_latest.json         --initialize  True        --input data/GO_term_analysis/toy_example_w_contaminated.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 10         --output_file data/GO_term_analysis/LLM_processed_toy_example_w_contamination_mixtral_latest\n",
      "--config ./jsonFiles/toyexample_llama2_70b.json         --initialize  True        --input data/GO_term_analysis/toy_example_w_contaminated.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 10         --output_file data/GO_term_analysis/LLM_processed_toy_example_w_contamination_llama2_70b\n",
      "--config ./jsonFiles/toyexample_llama2_7b.json         --initialize  True        --input data/GO_term_analysis/toy_example_w_contaminated.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 10         --output_file data/GO_term_analysis/LLM_processed_toy_example_w_contamination_llama2_7b\n",
      "--config ./jsonFiles/toyexample_gemini_pro.json         --initialize  True        --input data/GO_term_analysis/toy_example_w_contaminated.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 10         --output_file data/GO_term_analysis/LLM_processed_toy_example_w_contamination_gemini_pro\n",
      "--config ./jsonFiles/toyexample_mistral_7b.json         --initialize  True        --input data/GO_term_analysis/toy_example_w_contaminated.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 10         --output_file data/GO_term_analysis/LLM_processed_toy_example_w_contamination_mistral_7b\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from glob import glob\n",
    "\n",
    "\n",
    "initialize = True \n",
    "input_file = 'data/GO_term_analysis/toy_example_w_contaminated.csv'\n",
    "input_sep = constant.GO_FILE_SEP\n",
    "set_index = constant.GO_INDEX_COL  \n",
    "gene_column = constant.GO_GENE_COL \n",
    "gene_sep = ' '\n",
    "\n",
    "## create a param file \n",
    "configs = glob('./jsonFiles/toyexample_*.json')\n",
    "params = []\n",
    "for conf_file in configs:\n",
    "    model_names = '_'.join(conf_file.split('/')[-1].split('.')[0].split('_')[1:])\n",
    "    # print(model_names)\n",
    "    out_file = f'data/GO_term_analysis/LLM_processed_toy_example_w_contamination_{model_names}'  \n",
    "    param = f\"--config {conf_file} \\\n",
    "        --initialize  {initialize}\\\n",
    "        --input {input_file} \\\n",
    "        --input_sep  '{input_sep}'\\\n",
    "        --set_index {set_index} \\\n",
    "        --gene_column {gene_column}\\\n",
    "        --gene_sep '{gene_sep}' \\\n",
    "        --start 0 \\\n",
    "        --end 10 \\\n",
    "        --output_file {out_file}\"\n",
    "    print(param)\n",
    "    params.append(param)\n",
    "\n",
    "with open('toy_example_params.txt', 'w') as f:\n",
    "    for p in params:\n",
    "        f.write(p+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_4_50perc_contaminated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:46<06:58, 46.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:15<09:33, 71.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1749\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:16<07:47, 66.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [04:14<06:19, 63.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1690\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [04:49<04:24, 53.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1871\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [05:50<03:42, 55.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1782\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [06:31<02:33, 51.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [07:21<01:41, 50.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [07:54<00:45, 45.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1479\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:18<00:00, 49.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264\n",
      "Saved progress for 10 genesets\n",
      "gpt_4_100perc_contaminated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:02<09:18, 62.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1644\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:31<10:24, 78.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:28<07:59, 68.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1887\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [04:47<07:16, 72.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [05:17<04:46, 57.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1670\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [06:19<03:55, 58.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [06:37<02:16, 45.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [07:16<01:26, 43.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [07:46<00:39, 39.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:12<00:00, 49.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387\n",
      "Saved progress for 10 genesets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Define your own loop for running the pipeline\n",
    "## 12-18-2023: this loop is for run the default gene set and the contaminated gene sets \n",
    "## can modify this loop for different models or only run on default gene set\n",
    "\n",
    "##12-27-23: edited the prompt \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df = pd.read_csv(input_file, sep=input_sep, index_col=set_index)\n",
    "    \n",
    "    if 'gpt' in model:\n",
    "        name_fix = '_'.join(model.split('-')[:2])\n",
    "    else:\n",
    "        name_fix = model.replace(':', '_')\n",
    "    # column_prefix = name_fix + '_default'\n",
    "    \n",
    "    # if initialize:\n",
    "    #     # initialize the input file with llm names, analysis and score to None\n",
    "    #     df[f'{column_prefix} Name'] = None\n",
    "    #     df[f'{column_prefix} Analysis'] = None\n",
    "    #     df[f'{column_prefix} Score'] = None\n",
    "    # main(df)  ## run with the real set \n",
    "    \n",
    "    ## run the pipeline for contaiminated gene sets \n",
    "    contaminated_columns = [col for col in df.columns if col.endswith('contaminated_Genes')]\n",
    "    # print(contaminated_columns)\n",
    "    for col in contaminated_columns:\n",
    "        gene_column = col ## Note need to change the gene_column to the contaminated column\n",
    "        contam_prefix = '_'.join(col.split('_')[0:2])\n",
    "        \n",
    "        column_prefix = name_fix + '_' +contam_prefix\n",
    "        print(column_prefix)\n",
    "\n",
    "        if initialize:\n",
    "            # initialize the input file with llm names, analysis and score to None\n",
    "            df[f'{column_prefix} Name'] = None\n",
    "            df[f'{column_prefix} Analysis'] = None\n",
    "            df[f'{column_prefix} Score'] = None\n",
    "        main(df)\n",
    "    df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write a critical analysis of the biological processes performed by this system of interacting proteins.\n",
      "Base your analysis on prior knowledge available in your training data.\n",
      "After completing your analysis, propose a brief and detailed name for the most prominent biological process performed by the system.\n",
      "    \n",
      "After completing your analysis, please also assign a confidence score to the process name you selected.\n",
      "This score should follow the name in parentheses and range from 0.00 to 1.00. A score of 0.00 indicates the lowest confidence,\n",
      "while 1.00 reflects the highest confidence. This score helps gauge how accurately the chosen name represents the functions and activities\n",
      "within the system of interacting proteins. When determining your score, consider the proportion of genes in the protein system that participate\n",
      "in the identified biological process. For instance, if you select \"Ribosome biogenesis\" as the process name but only a few genes in the system \n",
      "contribute to this process, the score should be lower compared to a scenario where a majority of the genes are involved in \"Ribosome biogenesis\".\n",
      "     \n",
      "Put your chosen name at the top of the analysis as 'Process: <name>’.\n",
      "    \n",
      "Be concise, do not use unnecessary words.\n",
      "Be factual, do not editorialize.\n",
      "Be specific, avoid overly general statements such as 'the proteins are involved in various cellular processes'.\n",
      "Avoid listing facts about individual proteins. Instead, try to group proteins with similar functions and discuss their interplay, synergistyc or antagonistic effects and functional integration within the system.\n",
      "Also avoid choosing generic process names such as 'Cellular Signaling and Regulation'.\n",
      "If you cannot identify a prominent biological process for the proteins in the system, I want you to communicate this in you analysis and name the process: “System of unrelated proteins”. Provide a score of 0.00 for a \"System of unrelated proteins\".\n",
      "    \n",
      "To help you in your work, I am providing an example system of interacting proteins and the corresponding example analysis output.\n",
      "\n",
      "The example system of interacting proteins is:\n",
      "PDX1, SLC2A2, NKX6-1, GLP1, GCG.\n",
      "\n",
      "The example analysis output is:\n",
      "\n",
      "Process: Pancreatic development and glucose homeostasis (0.96)\n",
      "\n",
      "1. PDX1 is a homeodomain transcription factor involved in the specification of the early pancreatic epithelium and its subsequent differentiation. \n",
      "It activates the transcription of several genes including insulin, somatostatin, glucokinase and glucose transporter type 2. \n",
      "It is essential for maintenance of the normal hormone-producing phenotype in the pancreatic beta-cell. \n",
      "In pancreatic acinar cells, forms a complex with PBX1b and MEIS2b and mediates the activation of the ELA1 enhancer.\n",
      "\n",
      "2. NKX6-1 is also a transcription factor involved in the development of pancreatic beta-cells during the secondary transition. \n",
      "Together with NKX2-2 and IRX3, controls the generation of motor neurons in the neural tube and belongs to the neural progenitor \n",
      "factors induced by Sonic Hedgehog (SHH) signals.\n",
      "\n",
      "3.GCG and GLP1, respectively glucagon and glucagon-like peptide 1, are involved in glucose metabolism and homeostasis. \n",
      "GCG raises blood glucose levels by promoting gluconeogenesis and is the counter regulatory hormone of Insulin. \n",
      "GLP1 is a potent stimulator of Glucose-Induced Insulin Secretion (GSIS). Plays roles in gastric motility and suppresses blood glucagon levels. \n",
      "Promotes growth of the intestinal epithelium and pancreatic islet mass both by islet neogenesis and islet cell proliferation.\n",
      "\n",
      "4. SLC2A2, also known as GLUT2, is a facilitative hexose transporter. In hepatocytes, it mediates bi-directional transport of glucose accross the plasma membranes, \n",
      "while in the pancreatic beta-cell, it is the main transporter responsible for glucose uptake and part of the cell's glucose-sensing mechanism. \n",
      "It is involved in glucose transport in the small intestine and kidney too.\n",
      "\n",
      "To summarize, the genes in this set are involved in the specification, differentiation, growth and functionality of the pancreas, \n",
      "with a particular emphasis on the pancreatic beta-cell. Particularly, the architecture of the pancreatic islet ensures proper glucose sensing \n",
      "and homeostasis via a number of different hormones and receptors that can elicit both synergistic and antagonistic effects in the pancreas itself and other peripheral tissues.\n",
      "    \n",
      "\n",
      "Here are the interacting proteins:\n",
      "\n",
      "Proteins: GARNL3, OR1J1, SPP2, USP17L2, ARMH3, ANKRD13A, HELZ, OR10J1, C9orf78, FAM174B, SLC25A29, CELA2B, TUBA3D, DRP2, SLC35F6, ADAMTSL4, MRPL58, CCHCR1, CHRNA2, ADAM17, CLIP4, PSPH, PTPN4, TCEA3, HAL, CEP112, HTR3C, CDC26, MUC2, GALNT12, PGAP1, EID2, VPS39, UBE2I, XK, ZNF268, MTCL1, RTP4, C19orf18, ITGB1, RAD1, PSTK, MTCP1, SF3A3, CAMK2A, IFNL4, NLRP2, PTGES3, MTRR, OPA3, GPR34, CCDC134, BAP1, CLN3, SHH, HCAR1, WDR54, PIK3R6, PCDH18, SSU72, RTTN.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_file, sep=input_sep, index_col=set_index)\n",
    "df = df.loc['GO:1990874', :]\n",
    "\n",
    "genes = df['100perc_contaminated_Genes'].split(' ')\n",
    "print(make_user_prompt_with_score(genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 14)\n",
      "mixtral_latest_default\n",
      "3\n",
      "Genes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using server model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:55<04:50,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using server model\n",
      "Encountering server issue 502. Retrying in  10  seconds\n",
      "Encountering server issue 502. Retrying in  20  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [04:18<04:51, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using server model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:17<00:00,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50perc_contaminated_Genes\n",
      "mixtral_latest_50perc_contaminated\n",
      "4\n",
      "100perc_contaminated_Genes\n",
      "mixtral_latest_100perc_contaminated\n",
      "42\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check if there is any None in the analysis column, then rerun the pipeline\n",
    "\n",
    "## rerun the rows that fails mixtral     \n",
    "config_file = './jsonFiles/model_comparison_mixtral_latest.json'   # replace with your actual config file \n",
    "input_file = 'data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_mixtral_latest_0_50.tsv' # replace with your actual input file\n",
    "input_sep = '\\t'  # replace with the separator\n",
    "set_index = 'GO'  # replace with your column name that you want to set as index or None\n",
    "gene_column = 'Genes' \n",
    "initialize = False \n",
    "output_file = 'data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_mixtral_latest_0_50'\n",
    "\n",
    "context = config['CONTEXT']\n",
    "model = config['MODEL']\n",
    "temperature = config['TEMP']\n",
    "max_tokens = config['MAX_TOKENS']\n",
    "LOG_FILE = config['LOG_NAME']+'_0_50_log.json'\n",
    "SEED = constant.SEED\n",
    "\n",
    "df = pd.read_csv(input_file, sep=input_sep, index_col=set_index)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "if '-' in model:\n",
    "    name_fix = '_'.join(model.split('-')[:2])\n",
    "else:\n",
    "    name_fix = model.replace(':', '_')\n",
    "\n",
    "    \n",
    "column_prefix = name_fix + '_default' #this is default\n",
    "print(column_prefix)\n",
    "## find number of nones in the analysis column\n",
    "print(df[f'{column_prefix} Analysis'].isna().sum())\n",
    "\n",
    "gene_column = constant.GO_GENE_COL\n",
    "print(gene_column)\n",
    "\n",
    "if initialize:\n",
    "    # initialize the input file with llm names, analysis and score to None\n",
    "    df[f'{column_prefix} Name'] = None\n",
    "    df[f'{column_prefix} Analysis'] = None\n",
    "    df[f'{column_prefix} Score'] = None\n",
    "main(df)  ## run with the real set \n",
    "\n",
    "## run the pipeline for contaiminated gene sets \n",
    "contaminated_columns = [col for col in df.columns if col.endswith('contaminated_Genes')]\n",
    "# print(contaminated_columns)\n",
    "for col in contaminated_columns:\n",
    "    gene_column = col ## Note need to change the gene_column to the contaminated column\n",
    "    print(gene_column)\n",
    "    contam_prefix = '_'.join(col.split('_')[0:2])\n",
    "    column_prefix = name_fix + '_' +contam_prefix\n",
    "    print(column_prefix)\n",
    "    print(df[f'{column_prefix} Analysis'].isna().sum())\n",
    "    if initialize:\n",
    "        # initialize the input file with llm names, analysis and score to None\n",
    "        df[f'{column_prefix} Name'] = None\n",
    "        df[f'{column_prefix} Analysis'] = None\n",
    "        df[f'{column_prefix} Score'] = None\n",
    "    # main(df)\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Genes', '50perc_contaminated_Genes', '100perc_contaminated_Genes']\n",
      "gpt_4_default\n",
      "Accessing OpenAI API\n",
      "2046\n",
      "Success for Genes gpt_4.\n",
      "GPT_Fingerprint for Genes: fp_3905aa4f79\n",
      "gpt_4_50perc_contaminated\n",
      "Accessing OpenAI API\n",
      "1804\n",
      "Success for 50perc_contaminated_Genes gpt_4.\n",
      "GPT_Fingerprint for 50perc_contaminated_Genes: fp_3905aa4f79\n",
      "gpt_4_100perc_contaminated\n",
      "Accessing OpenAI API\n",
      "1760\n",
      "Success for 100perc_contaminated_Genes gpt_4.\n",
      "GPT_Fingerprint for 100perc_contaminated_Genes: fp_3905aa4f79\n",
      "save file for  gpt-4-1106-preview\n",
      "gemini_pro_default\n",
      "Using Google Gemini API\n",
      "Success for Genes gemini_pro.\n",
      "gemini_pro_50perc_contaminated\n",
      "Using Google Gemini API\n",
      "Success for 50perc_contaminated_Genes gemini_pro.\n",
      "gemini_pro_100perc_contaminated\n",
      "Using Google Gemini API\n",
      "Success for 100perc_contaminated_Genes gemini_pro.\n",
      "save file for  gemini-pro\n",
      "mistral_7b_default\n",
      "Using server model\n",
      "Encountering server issue 502. Retrying in  10  seconds\n",
      "Success for Genes mistral_7b.\n",
      "mistral_7b_50perc_contaminated\n",
      "Using server model\n",
      "Encountering server issue 502. Retrying in  10  seconds\n",
      "Success for 50perc_contaminated_Genes mistral_7b.\n",
      "mistral_7b_100perc_contaminated\n",
      "Using server model\n",
      "Success for 100perc_contaminated_Genes mistral_7b.\n",
      "save file for  mistral:7b\n",
      "mixtral_latest_default\n",
      "Using server model\n",
      "Encountering server issue 502. Retrying in  10  seconds\n",
      "Encountering server issue 502. Retrying in  20  seconds\n",
      "Encountering server issue 502. Retrying in  40  seconds\n",
      "Encountering server issue 502. Retrying in  80  seconds\n",
      "Encountering server issue 502. Retrying in  160  seconds\n",
      "Error for query gene set Genes: Error: Max retries exceeded, last response error was: 502\n",
      "mixtral_latest_50perc_contaminated\n",
      "Using server model\n",
      "Encountering server issue 502. Retrying in  10  seconds\n",
      "Encountering server issue 502. Retrying in  20  seconds\n",
      "Success for 50perc_contaminated_Genes mixtral_latest.\n",
      "mixtral_latest_100perc_contaminated\n",
      "Using server model\n",
      "Success for 100perc_contaminated_Genes mixtral_latest.\n",
      "save file for  mixtral:latest\n",
      "llama2_7b_default\n",
      "Using server model\n",
      "Success for Genes llama2_7b.\n",
      "llama2_7b_50perc_contaminated\n",
      "Using server model\n",
      "Encountering server issue 502. Retrying in  10  seconds\n",
      "Success for 50perc_contaminated_Genes llama2_7b.\n",
      "llama2_7b_100perc_contaminated\n",
      "Using server model\n",
      "Encountering server issue 502. Retrying in  10  seconds\n",
      "Success for 100perc_contaminated_Genes llama2_7b.\n",
      "save file for  llama2:7b\n"
     ]
    }
   ],
   "source": [
    "# add a gene set == 100 and run the analysis. add to the toy example file\n",
    "selected_go_terms = pd.read_csv('data/GO_term_analysis/1000_selected_go_contaminated.csv')\n",
    "\n",
    "# only one that has gene count == 100\n",
    "gene_count100 = selected_go_terms[selected_go_terms['Gene_Count'] == 100].reset_index(drop=True)\n",
    "\n",
    "model_options = ['gpt-4-1106-preview','gemini-pro','mistral:7b', 'mixtral:latest', 'llama2:7b']\n",
    "contaminated_columns = [col for col in gene_count100.columns if col.endswith('contaminated_Genes')]\n",
    "columns = [constant.GO_GENE_COL] + contaminated_columns\n",
    "print(columns)\n",
    "for model in model_options:\n",
    "    df = gene_count100.copy()\n",
    "    for col in columns:\n",
    "\n",
    "        gene_column = col\n",
    "        genes = gene_count100[gene_column].iloc[0].split(' ')\n",
    "        prompt = make_user_prompt_with_score(genes)\n",
    "\n",
    "        if '-' in model:\n",
    "            name_fix = '_'.join(model.split('-')[:2])\n",
    "        else:\n",
    "            name_fix = model.replace(':', '_')\n",
    "        LOG_FILE = config['LOG_NAME']+f'_{name_fix}'+'_log.json'\n",
    "        if col == constant.GO_GENE_COL:\n",
    "            column_prefix = name_fix + '_default'\n",
    "        else:\n",
    "            contam_prefix = '_'.join(col.split('_')[0:2])\n",
    "            column_prefix = name_fix + '_' +contam_prefix\n",
    "        print(column_prefix)\n",
    "        finger_print = None\n",
    "        if 'gpt' in model:\n",
    "            print(\"Accessing OpenAI API\")\n",
    "            analysis, finger_print = openai_chat(context, prompt, model, temperature, max_tokens, rate_per_token, LOG_FILE, DOLLAR_LIMIT, SEED)\n",
    "        elif model.startswith('gemini'):\n",
    "            print(\"Using Google Gemini API\")\n",
    "            analysis, error_message = query_genai_model(f\"{context}\\n{prompt}\", model, temperature, max_tokens, LOG_FILE)\n",
    "        else: \n",
    "            print(\"Using server model\")\n",
    "            analysis, error_message= server_model_chat(context, prompt, model, temperature, max_tokens,LOG_FILE, SEED)\n",
    "        if analysis:\n",
    "            # print(analysis)\n",
    "            llm_name, llm_score, llm_analysis = process_analysis(analysis)\n",
    "            # clean up the score and return float\n",
    "            try:\n",
    "                llm_score_value =  float(re.sub(\"[^0-9.-]\", \"\", llm_score))\n",
    "            except ValueError:\n",
    "                llm_score_value = llm_score\n",
    "            df.loc[0,f'{column_prefix } Name'] = llm_name\n",
    "            df.loc[0,f'{column_prefix } Analysis'] = llm_analysis\n",
    "            df.loc[0,f'{column_prefix } Score'] = llm_score_value\n",
    "            print(f'Success for {gene_column} {name_fix}.')\n",
    "            if finger_print:\n",
    "                print(f'GPT_Fingerprint for {gene_column}: {finger_print}')\n",
    "        else:    \n",
    "            print(f'Error for query gene set {gene_column}: {error_message}')\n",
    "    df.to_csv(f'data/GO_term_analysis/add_on_LLM_processed_toy_example_w_contamination_{name_fix}.tsv', index=False,sep='\\t')\n",
    "    print('save file for ', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixtral_latest_default\n",
      "Encountering server issue 502. Retrying in  10  seconds\n",
      "Encountering server issue 502. Retrying in  20  seconds\n",
      "Encountering server issue 502. Retrying in  40  seconds\n",
      "Encountering server issue 502. Retrying in  80  seconds\n",
      "Encountering server issue 502. Retrying in  160  seconds\n",
      "Encountering server issue 502. Retrying in  10  seconds\n",
      "Encountering server issue 502. Retrying in  20  seconds\n",
      "Encountering server issue 502. Retrying in  40  seconds\n",
      "Encountering server issue 502. Retrying in  80  seconds\n",
      "Encountering server issue 502. Retrying in  160  seconds\n",
      "Error for query gene set 100perc_contaminated_Genes: Error: Max retries exceeded, last response error was: 502\n",
      "                                                        Genes  Gene_Count  \\\n",
      "GO                                                                          \n",
      "GO:0090100  SOX11 GDF5 TGFB3 NOTCH1 AMH TNXB FERMT1 TGFBR3...         100   \n",
      "\n",
      "                                             Term_Description  \\\n",
      "GO                                                              \n",
      "GO:0090100  positive regulation of transmembrane receptor ...   \n",
      "\n",
      "                                    50perc_contaminated_Genes  \\\n",
      "GO                                                              \n",
      "GO:0090100  CDH5 BMP6 SH2B1 MSX2 ING2 ZC3H3 BMP4 TSC22D1 S...   \n",
      "\n",
      "                                   100perc_contaminated_Genes  \\\n",
      "GO                                                              \n",
      "GO:0090100  KLRC4-KLRK1 SMC1A POU5F1 RAB1A TMPRSS4 COLGALT...   \n",
      "\n",
      "                      mixtral_latest_50perc_contaminated Name  \\\n",
      "GO                                                              \n",
      "GO:0090100   Regulation of developmental processes and cel...   \n",
      "\n",
      "                  mixtral_latest_50perc_contaminated Analysis  \\\n",
      "GO                                                              \n",
      "GO:0090100  The system of interacting proteins consists of...   \n",
      "\n",
      "            mixtral_latest_50perc_contaminated Score  \\\n",
      "GO                                                     \n",
      "GO:0090100                                      0.85   \n",
      "\n",
      "                     mixtral_latest_100perc_contaminated Name  \\\n",
      "GO                                                              \n",
      "GO:0090100   Regulation of gene expression and chromatin o...   \n",
      "\n",
      "                 mixtral_latest_100perc_contaminated Analysis  \\\n",
      "GO                                                              \n",
      "GO:0090100  The proteins in this system are primarily invo...   \n",
      "\n",
      "            mixtral_latest_100perc_contaminated Score  \n",
      "GO                                                     \n",
      "GO:0090100                                       0.92  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "df = pd.read_csv('data/GO_term_analysis/add_on_LLM_processed_toy_example_w_contamination_mixtral_latest.tsv', sep='\\t', index_col=constant.GO_INDEX_COL)\n",
    "# rerun the pipeline for 'Genes'\n",
    "genes = df['Genes'].iloc[0].split(' ')\n",
    "prompt = make_user_prompt_with_score(genes)\n",
    "model = 'mixtral:latest'\n",
    "name_fix = '_'.join(model.split(':')[:2])\n",
    "column_prefix = name_fix + '_default'\n",
    "print(column_prefix)\n",
    "LOG_FILE = config['LOG_NAME']+f'_{name_fix}'+'_log.json'\n",
    "analysis, err  = server_model_chat(context, prompt, model, temperature, max_tokens,LOG_FILE, SEED)\n",
    "if analysis:\n",
    "    llm_name, llm_score, llm_analysis = process_analysis(analysis)\n",
    "    llm_score_value =  float(re.sub(\"[^0-9.-]\", \"\", llm_score))\n",
    "    df.loc[0,f'{column_prefix } Name'] = llm_name\n",
    "    df.loc[0,f'{column_prefix } Analysis'] = llm_analysis\n",
    "    df.loc[0,f'{column_prefix } Score'] = llm_score_value\n",
    "    print(f'Success for {gene_column} {name_fix}.')\n",
    "else:\n",
    "    time.sleep(20)\n",
    "    #retry\n",
    "    analysis, err  = server_model_chat(context, prompt, model, temperature, max_tokens,LOG_FILE, SEED)\n",
    "    if analysis:\n",
    "        llm_name, llm_score, llm_analysis = process_analysis(analysis)\n",
    "        llm_score_value =  float(re.sub(\"[^0-9.-]\", \"\", llm_score))\n",
    "        df.loc[0,f'{column_prefix } Name'] = llm_name\n",
    "        df.loc[0,f'{column_prefix } Analysis'] = llm_analysis\n",
    "        df.loc[0,f'{column_prefix } Score'] = llm_score_value\n",
    "        print(f'Success for {gene_column} {name_fix}.')\n",
    "    else:\n",
    "        print(f'Error for query gene set {gene_column}: {err}')\n",
    "    \n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--config ./jsonFiles/model_comparison_mixtral_latest.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 0             --end 50             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_mixtral_latest_0_50\n",
      "--config ./jsonFiles/model_comparison_gpt4.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 0             --end 50             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gpt4_0_50\n",
      "--config ./jsonFiles/model_comparison_gemini_pro.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 0             --end 50             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gemini_pro_0_50\n",
      "--config ./jsonFiles/model_comparison_mixtral_latest.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 50             --end 100             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_mixtral_latest_50_100\n",
      "--config ./jsonFiles/model_comparison_gpt4.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 50             --end 100             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gpt4_50_100\n",
      "--config ./jsonFiles/model_comparison_gemini_pro.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 50             --end 100             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gemini_pro_50_100\n",
      "number of params:  6\n"
     ]
    }
   ],
   "source": [
    "## set up parameters for running the pipeline for every 50 rows\n",
    "import os \n",
    "from glob import glob\n",
    "# Define start, step, and end values\n",
    "start = 0\n",
    "step = 50\n",
    "end = 100\n",
    "\n",
    "# Create a range list\n",
    "range_list = list(range(start, end + step, step))\n",
    "\n",
    "# Create tuples for each consecutive pair in the list\n",
    "tuple_list = [(range_list[i], range_list[i+1]) for i in range(len(range_list)-1)]\n",
    "\n",
    "\n",
    "initialize = True \n",
    "input_file = 'data/GO_term_analysis/model_comparison_terms.csv'\n",
    "input_sep = constant.GO_FILE_SEP\n",
    "set_index = constant.GO_INDEX_COL  \n",
    "gene_column = constant.GO_GENE_COL \n",
    "gene_sep = ' '\n",
    "\n",
    "## create a param file \n",
    "configs = glob('./jsonFiles/model_comparison_*.json')\n",
    "params = []\n",
    "for start, end in tuple_list:\n",
    "    for conf_file in configs:\n",
    "        model_names = '_'.join(conf_file.split('/')[-1].split('.')[0].split('_')[1:])\n",
    "        # print(model_names)\n",
    "        \n",
    "        out_file = f'data/GO_term_analysis/model_compare/LLM_processed_model_compare_{model_names}_{start}_{end}'  \n",
    "        param = f\"--config {conf_file} \\\n",
    "            --initialize  {initialize}\\\n",
    "            --input {input_file} \\\n",
    "            --input_sep  '{input_sep}'\\\n",
    "            --set_index {set_index} \\\n",
    "            --gene_column {gene_column}\\\n",
    "            --gene_sep '{gene_sep}' \\\n",
    "            --start {start} \\\n",
    "            --end {end} \\\n",
    "            --output_file {out_file}\"\n",
    "        print(param)\n",
    "        params.append(param)\n",
    "print('number of params: ', len(params))\n",
    "    \n",
    "\n",
    "with open('model_compare_params.txt', 'w') as f:\n",
    "    for p in params:\n",
    "        f.write(p+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkout and combine the output from the batch run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "### sanity check code along the way\n",
    "processed_files = glob('data/GO_term_analysis/LLM_processed_selected_go_terms*.tsv')\n",
    "\n",
    "for file in processed_files:\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    df.set_index('GO', inplace=True)\n",
    "    ranges = file.split('/')[-1].split('.')[0].split('_')[5:7]\n",
    "    with open(f'data/GO_term_analysis/LLM_response_go_terms_{ranges[0]}_{ranges[1]}.json') as fp:\n",
    "        llm_response_dict = json.load(fp)\n",
    "    for go_term, row in df.iterrows():\n",
    "        if llm_response_dict[go_term] == 'NO ANALYSIS':\n",
    "            print(file.split('/')[-1])\n",
    "            print(f'No analysis for {go_term}')\n",
    "            continue\n",
    "        else:\n",
    "            llm_analysis = llm_response_dict[go_term].split('\\n', 2)[2]\n",
    "            if df.loc[go_term, 'LLM Analysis'] != llm_analysis:\n",
    "                print(f'LLM analysis for {go_term} is different')\n",
    "            \n",
    "    df.reset_index(inplace=True)\n",
    "#     # print(ranges)\n",
    "    print(df.shape)\n",
    "\n",
    "    \n",
    "combined_df = pd.concat([pd.read_csv(f, sep = '\\t') for f in processed_files])\n",
    "print(combined_df.shape)\n",
    "print('Any duplicated GO: ',combined_df['GO'].duplicated().sum())\n",
    "print('Any NAs in the LLM res: ', combined_df['LLM Name'].isna().sum())\n",
    "print('Any duplicated LLM analysis: ', combined_df['LLM Analysis'].duplicated(keep=False).sum())\n",
    "\n",
    "combined_df.to_csv('data/GO_term_analysis/LLM_processed_selected_1000_go_terms.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
