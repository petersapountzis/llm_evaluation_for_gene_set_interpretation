{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model comparison\n",
    "\n",
    "**gpt model**\n",
    "\n",
    "gpt4-1103-preview\n",
    "\n",
    "**local model on server**\n",
    "new available models through api: https://api.llm.ideker.ucsd.edu/api/chat\n",
    "\n",
    "available models:\n",
    "\n",
    "| NAME           | ID           | SIZE   |\n",
    "|----------------|--------------|--------|\n",
    "| llama2:70b     | c3a7af098300 | 38 GB  |\n",
    "| llama2:7b      | fe938a131f40 | 3.8 GB |\n",
    "| llama2:latest  | fe938a131f40 | 3.8 GB |\n",
    "| mistral:7b     | 4d9f4b269c33 | 4.1 GB |\n",
    "| mixtral:latest | 99a9202f8a7a | 26 GB  |\n",
    "\n",
    "\n",
    "**API for calling Google Gemini pro**\n",
    "\n",
    "GO TO: https://makersuite.google.com/app/apikey to get the apikey for gemini pro\n",
    "\n",
    "export GOOGLEAI_KEY = xxxx\n",
    "\n",
    "model = 'gemini-pro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "from utils.openai_query import openai_chat\n",
    "from utils.prompt_factory import make_user_prompt_with_score\n",
    "from utils.server_model_query import server_model_chat\n",
    "from utils.llm_analysis_utils import process_analysis, save_progress\n",
    "from utils.genai_query import query_genai_model\n",
    "from tqdm import tqdm\n",
    "import constant\n",
    "import openai\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default run is using GPT4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load variables\n",
    "initialize = True # if True, then initialize the input table with llm names, analysis and score to None \n",
    "# Replace with your actual values\n",
    "config_file = './jsonFiles/model_comparison_gpt_4.json'  # replace with your actual config file \n",
    "input_file = 'data/GO_term_analysis/model_comparison_terms.csv' # replace with your actual input file\n",
    "input_sep = ','  # replace with the separator\n",
    "set_index = 'GO'  # replace with your column name that you want to set as index or None\n",
    "gene_column = 'Genes'  # replace with your actual column name for the gene list\n",
    "gene_sep = ' '  # replace with your actual separator\n",
    "gene_features = None  # replace with your path to the gene features or None if you don't want to include in the prompt\n",
    "direct = False # if True, then the prompt will be a direct sentence asking for a name and analysis from the gene set, otherwise default or customized prompt\n",
    "out_file = 'data/GO_term_analysis/trial'  # replace with your actual output file name\n",
    "\n",
    "customized_prompt = False # if True, then the prompt will be the custom prompt, if False, then the prompt will use default\n",
    "\n",
    "# load the config file\n",
    "with open(config_file) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "if customized_prompt:\n",
    "    # make sure the file exist \n",
    "    if os.path.isfile(config['CUSTOM_PROMPT_FILE']):\n",
    "        with open(config['CUSTOM_PROMPT_FILE'], 'r') as f: # replace with your actual customized prompt file\n",
    "            customized_prompt = f.read()\n",
    "            assert len(customized_prompt) > 1, \"Customized prompt is empty\"\n",
    "    else:\n",
    "        print(\"Customized prompt file does not exist\")\n",
    "        customized_prompt = None\n",
    "else:\n",
    "    customized_prompt = None\n",
    "\n",
    "# Load OpenAI key, context, and model used \n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "context = config['CONTEXT']\n",
    "model = config['MODEL']\n",
    "temperature = config['TEMP']\n",
    "max_tokens = config['MAX_TOKENS']\n",
    "if model.startswith('gpt'):\n",
    "    rate_per_token = config['RATE_PER_TOKEN']\n",
    "    DOLLAR_LIMIT = config['DOLLAR_LIMIT']\n",
    "LOG_FILE = config['LOG_NAME']+'_log.json'\n",
    "\n",
    "SEED = constant.SEED\n",
    "column_prefix = model.split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the logger so it create a new one for each model run\n",
    "def get_logger(filename):\n",
    "    logger = logging.getLogger(filename)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(filename)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def main(df):\n",
    "    analysis_dict  = {}\n",
    "\n",
    "    logger = get_logger(f'{out_file}.log')\n",
    "\n",
    "    i = 0 #used for track progress and saving the file\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        #only process None rows \n",
    "        if pd.notna(row[f'{column_prefix} Analysis']):\n",
    "            continue\n",
    "        \n",
    "        gene_data = row[gene_column]\n",
    "        # if gene_data is not a string, then skip\n",
    "        if type(gene_data) != str:\n",
    "            \n",
    "            logger.warning(f'Gene set {idx} is not a string, skipping')\n",
    "            continue\n",
    "        genes = gene_data.split(gene_sep)\n",
    "        \n",
    "        if len(genes) >1000:\n",
    "            logger.warning(f'Gene set {idx} is too big, skipping')\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            prompt = make_user_prompt_with_score(genes)\n",
    "            # print(prompt)\n",
    "            finger_print = None\n",
    "            if model.startswith('gpt'):\n",
    "                print(\"Accessing OpenAI API\")\n",
    "                analysis, finger_print = openai_chat(context, prompt, model, temperature, max_tokens, rate_per_token, LOG_FILE, DOLLAR_LIMIT, SEED)\n",
    "            elif model.startswith('gemini'):\n",
    "                print(\"Using Google Gemini API\")\n",
    "                analysis, error_message = query_genai_model(f\"{context}\\n{prompt}\", model, temperature, max_tokens, LOG_FILE) \n",
    "            else:\n",
    "                print(\"Using server model\")\n",
    "                analysis, error_message= server_model_chat(context, prompt, model, temperature, max_tokens,LOG_FILE, SEED)\n",
    "\n",
    "            \n",
    "            if analysis:\n",
    "                # print(analysis)\n",
    "                llm_name, llm_score, llm_analysis = process_analysis(analysis)\n",
    "                # clean up the score and return float\n",
    "                try:\n",
    "                    llm_score_value =  float(re.sub(\"[^0-9.-]\", \"\", llm_score))\n",
    "                except ValueError:\n",
    "                    llm_score_value = llm_score\n",
    "            \n",
    "                \n",
    "                df.loc[idx, f'{column_prefix} Name'] = llm_name\n",
    "                df.loc[idx, f'{column_prefix} Analysis'] = llm_analysis\n",
    "                df.loc[idx, f'{column_prefix} Score'] = llm_score_value\n",
    "                analysis_dict[f'{idx}_{column_prefix}'] = analysis\n",
    "                # Log success with fingerprint\n",
    "                logger.info(f'Success for {idx} {column_prefix}.')\n",
    "                if finger_print:\n",
    "                    logger.info(f'GPT_Fingerprint for {idx}: {finger_print}')\n",
    "                    \n",
    "            else:\n",
    "                logger.error(f'Error for query gene set {idx}: {error_message}')\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error for {idx}: {e}')\n",
    "            continue\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            save_progress(df, analysis_dict, out_file)\n",
    "            # df.to_csv(f'{out_file}.tsv', sep='\\t', index=True)\n",
    "            print(f\"Saved progress for {i} genesets\")\n",
    "    # save the final file\n",
    "    save_progress(df, analysis_dict, out_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--config ./jsonFiles/toyexample_mixtral_latest.json         --initialize  True        --input data/GO_term_analysis/toy_example_w_contaminated.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 10         --output_file data/GO_term_analysis/LLM_processed_toy_example_w_contamination_mixtral_latest\n",
      "--config ./jsonFiles/toyexample_llama2_70b.json         --initialize  True        --input data/GO_term_analysis/toy_example_w_contaminated.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 10         --output_file data/GO_term_analysis/LLM_processed_toy_example_w_contamination_llama2_70b\n",
      "--config ./jsonFiles/toyexample_llama2_7b.json         --initialize  True        --input data/GO_term_analysis/toy_example_w_contaminated.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 10         --output_file data/GO_term_analysis/LLM_processed_toy_example_w_contamination_llama2_7b\n",
      "--config ./jsonFiles/toyexample_gemini_pro.json         --initialize  True        --input data/GO_term_analysis/toy_example_w_contaminated.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 10         --output_file data/GO_term_analysis/LLM_processed_toy_example_w_contamination_gemini_pro\n",
      "--config ./jsonFiles/toyexample_mistral_7b.json         --initialize  True        --input data/GO_term_analysis/toy_example_w_contaminated.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 10         --output_file data/GO_term_analysis/LLM_processed_toy_example_w_contamination_mistral_7b\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from glob import glob\n",
    "\n",
    "\n",
    "initialize = True \n",
    "input_file = 'data/GO_term_analysis/toy_example_w_contaminated.csv'\n",
    "input_sep = constant.GO_FILE_SEP\n",
    "set_index = constant.GO_INDEX_COL  \n",
    "gene_column = constant.GO_GENE_COL \n",
    "gene_sep = ' '\n",
    "\n",
    "## create a param file \n",
    "configs = glob('./jsonFiles/toyexample_*.json')\n",
    "params = []\n",
    "for conf_file in configs:\n",
    "    model_names = '_'.join(conf_file.split('/')[-1].split('.')[0].split('_')[1:])\n",
    "    # print(model_names)\n",
    "    out_file = f'data/GO_term_analysis/LLM_processed_toy_example_w_contamination_{model_names}'  \n",
    "    param = f\"--config {conf_file} \\\n",
    "        --initialize  {initialize}\\\n",
    "        --input {input_file} \\\n",
    "        --input_sep  '{input_sep}'\\\n",
    "        --set_index {set_index} \\\n",
    "        --gene_column {gene_column}\\\n",
    "        --gene_sep '{gene_sep}' \\\n",
    "        --start 0 \\\n",
    "        --end 10 \\\n",
    "        --output_file {out_file}\"\n",
    "    print(param)\n",
    "    params.append(param)\n",
    "\n",
    "with open('toy_example_params.txt', 'w') as f:\n",
    "    for p in params:\n",
    "        f.write(p+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_4_50perc_contaminated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:46<06:58, 46.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:15<09:33, 71.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1749\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:16<07:47, 66.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [04:14<06:19, 63.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1690\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [04:49<04:24, 53.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1871\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [05:50<03:42, 55.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1782\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [06:31<02:33, 51.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [07:21<01:41, 50.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [07:54<00:45, 45.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1479\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:18<00:00, 49.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264\n",
      "Saved progress for 10 genesets\n",
      "gpt_4_100perc_contaminated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:02<09:18, 62.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1644\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:31<10:24, 78.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:28<07:59, 68.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1887\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [04:47<07:16, 72.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [05:17<04:46, 57.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1670\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [06:19<03:55, 58.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [06:37<02:16, 45.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [07:16<01:26, 43.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [07:46<00:39, 39.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520\n",
      "Accessing OpenAI API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:12<00:00, 49.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387\n",
      "Saved progress for 10 genesets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Define your own loop for running the pipeline\n",
    "## 12-18-2023: this loop is for run the default gene set and the contaminated gene sets \n",
    "## can modify this loop for different models or only run on default gene set\n",
    "\n",
    "##12-27-23: edited the prompt \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df = pd.read_csv(input_file, sep=input_sep, index_col=set_index)\n",
    "    \n",
    "    if 'gpt' in model:\n",
    "        name_fix = '_'.join(model.split('-')[:2])\n",
    "    else:\n",
    "        name_fix = model.replace(':', '_')\n",
    "    # column_prefix = name_fix + '_default'\n",
    "    \n",
    "    # if initialize:\n",
    "    #     # initialize the input file with llm names, analysis and score to None\n",
    "    #     df[f'{column_prefix} Name'] = None\n",
    "    #     df[f'{column_prefix} Analysis'] = None\n",
    "    #     df[f'{column_prefix} Score'] = None\n",
    "    # main(df)  ## run with the real set \n",
    "    \n",
    "    ## run the pipeline for contaiminated gene sets \n",
    "    contaminated_columns = [col for col in df.columns if col.endswith('contaminated_Genes')]\n",
    "    # print(contaminated_columns)\n",
    "    for col in contaminated_columns:\n",
    "        gene_column = col ## Note need to change the gene_column to the contaminated column\n",
    "        contam_prefix = '_'.join(col.split('_')[0:2])\n",
    "        \n",
    "        column_prefix = name_fix + '_' +contam_prefix\n",
    "        print(column_prefix)\n",
    "\n",
    "        if initialize:\n",
    "            # initialize the input file with llm names, analysis and score to None\n",
    "            df[f'{column_prefix} Name'] = None\n",
    "            df[f'{column_prefix} Analysis'] = None\n",
    "            df[f'{column_prefix} Score'] = None\n",
    "        main(df)\n",
    "    df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixtral_latest_default\n",
      "Genes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using server model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:34<00:00,  8.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50perc_contaminated_Genes\n",
      "mixtral_latest_50perc_contaminated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 15246.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100perc_contaminated_Genes\n",
      "mixtral_latest_100perc_contaminated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 16098.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check if there is any None in the analysis column, then rerun the pipeline\n",
    "\n",
    "initialize = False \n",
    "\n",
    "SEED = 42\n",
    "# model_options = ['gemini-pro','mistral:7b', 'mixtral:latest', 'llama2:7b', 'llama2:70b']\n",
    "model_options = ['mixtral:latest']  # llama2 7b has formatting issue, ingore and 70b is too big causing server issue\n",
    "input_sep = '\\t'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for m in model_options:\n",
    "        input_file\n",
    "        model = m\n",
    "        \n",
    "        if '-' in model:\n",
    "            name_fix = '_'.join(model.split('-')[:2])\n",
    "        else:\n",
    "            name_fix = model.replace(':', '_')\n",
    "        input_file = f'data/GO_term_analysis/LLM_processed_toy_example_w_contamination_{name_fix}.tsv' # replace with your actual input file\n",
    "        out_file = f'data/GO_term_analysis/LLM_processed_toy_example_w_contamination_{name_fix}'  # save to the same file name as the input file\n",
    "        LOG_FILE = config['LOG_NAME']+f'_{name_fix}'+'_log.json'\n",
    "\n",
    "        df = pd.read_csv(input_file, sep=input_sep, index_col=set_index)\n",
    "        # print(df.head())\n",
    "        column_prefix = name_fix + '_default' #this is default\n",
    "        print(column_prefix)\n",
    "        \n",
    "        gene_column = constant.GO_GENE_COL\n",
    "        print(gene_column)\n",
    "        if initialize:\n",
    "            # initialize the input file with llm names, analysis and score to None\n",
    "            df[f'{column_prefix} Name'] = None\n",
    "            df[f'{column_prefix} Analysis'] = None\n",
    "            df[f'{column_prefix} Score'] = None\n",
    "        main(df)  ## run with the real set \n",
    "        \n",
    "        ## run the pipeline for contaiminated gene sets \n",
    "        contaminated_columns = [col for col in df.columns if col.endswith('contaminated_Genes')]\n",
    "        # print(contaminated_columns)\n",
    "        for col in contaminated_columns:\n",
    "            gene_column = col ## Note need to change the gene_column to the contaminated column\n",
    "            print(gene_column)\n",
    "            contam_prefix = '_'.join(col.split('_')[0:2])\n",
    "            column_prefix = name_fix + '_' +contam_prefix\n",
    "            print(column_prefix)\n",
    "\n",
    "            if initialize:\n",
    "                # initialize the input file with llm names, analysis and score to None\n",
    "                df[f'{column_prefix} Name'] = None\n",
    "                df[f'{column_prefix} Analysis'] = None\n",
    "                df[f'{column_prefix} Score'] = None\n",
    "            main(df)\n",
    "            \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2_70b_default\n",
      "Genes\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using server model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:47<00:00,  9.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50perc_contaminated_Genes\n",
      "llama2_70b_50perc_contaminated\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using server model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:08<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100perc_contaminated_Genes\n",
      "llama2_70b_100perc_contaminated\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using server model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:34<00:00,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check if there is any None in the analysis column, then rerun the pipeline\n",
    "\n",
    "initialize = False\n",
    "\n",
    "SEED = 42\n",
    "# model_options = ['gemini-pro','mistral:7b', 'mixtral:latest', 'llama2:7b', 'llama2:70b']\n",
    "model_options = ['llama2:70b']  # llama2 7b has formatting issue, ingore and 70b is too big causing server issue\n",
    "input_sep = '\\t'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for m in model_options:\n",
    "        input_file\n",
    "        model = m\n",
    "        \n",
    "        if '-' in model:\n",
    "            name_fix = '_'.join(model.split('-')[:2])\n",
    "        else:\n",
    "            name_fix = model.replace(':', '_')\n",
    "        input_file = f'data/GO_term_analysis/LLM_processed_toy_example_w_contamination_{name_fix}.tsv'\n",
    "        \n",
    "        out_file = f'data/GO_term_analysis/LLM_processed_toy_example_w_contamination_{name_fix}'  # save to the same file name as the input file\n",
    "        LOG_FILE = config['LOG_NAME']+f'_{name_fix}'+'_log.json'\n",
    "        \n",
    "        df = pd.read_csv(input_file, sep=input_sep, index_col=set_index)\n",
    "        # print(df.head())\n",
    "        column_prefix = name_fix + '_default' #this is default\n",
    "        print(column_prefix)\n",
    "        \n",
    "        gene_column = constant.GO_GENE_COL\n",
    "        print(gene_column)\n",
    "        if initialize:\n",
    "            # initialize the input file with llm names, analysis and score to None\n",
    "            df[f'{column_prefix} Name'] = None\n",
    "            df[f'{column_prefix} Analysis'] = None\n",
    "            df[f'{column_prefix} Score'] = None\n",
    "        #if any none in the analysis column\n",
    "        print(df[f'{column_prefix} Analysis'].isna().sum())\n",
    "        main(df)  ## run with the real set \n",
    "        \n",
    "        ## run the pipeline for contaiminated gene sets \n",
    "        contaminated_columns = [col for col in df.columns if col.endswith('contaminated_Genes')]\n",
    "        # print(contaminated_columns)\n",
    "        for col in contaminated_columns:\n",
    "            gene_column = col ## Note need to change the gene_column to the contaminated column\n",
    "            print(gene_column)\n",
    "            contam_prefix = '_'.join(col.split('_')[0:2])\n",
    "            column_prefix = name_fix + '_' +contam_prefix\n",
    "            print(column_prefix)\n",
    "\n",
    "            if initialize:\n",
    "                # initialize the input file with llm names, analysis and score to None\n",
    "                df[f'{column_prefix} Name'] = None\n",
    "                df[f'{column_prefix} Analysis'] = None\n",
    "                df[f'{column_prefix} Score'] = None\n",
    "            print(df[f'{column_prefix} Analysis'].isna().sum())\n",
    "            main(df)\n",
    "            \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparison_gpt_4\n",
      "--config ./jsonFiles/model_comparison_gpt_4.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 0             --end 50             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gpt_4_0_50\n",
      "comparison_mixtral_latest\n",
      "--config ./jsonFiles/model_comparison_mixtral_latest.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 0             --end 50             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_mixtral_latest_0_50\n",
      "comparison_gemini_pro\n",
      "--config ./jsonFiles/model_comparison_gemini_pro.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 0             --end 50             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gemini_pro_0_50\n",
      "comparison_gpt_4\n",
      "--config ./jsonFiles/model_comparison_gpt_4.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 50             --end 100             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gpt_4_50_100\n",
      "comparison_mixtral_latest\n",
      "--config ./jsonFiles/model_comparison_mixtral_latest.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 50             --end 100             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_mixtral_latest_50_100\n",
      "comparison_gemini_pro\n",
      "--config ./jsonFiles/model_comparison_gemini_pro.json             --initialize  True            --input data/GO_term_analysis/model_comparison_terms.csv             --input_sep  ','            --set_index GO             --gene_column Genes            --gene_sep ' '             --start 50             --end 100             --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gemini_pro_50_100\n",
      "number of params:  6\n"
     ]
    }
   ],
   "source": [
    "## set up parameters for running the pipeline for every 50 rows\n",
    "import os \n",
    "from glob import glob\n",
    "# Define start, step, and end values\n",
    "start = 0\n",
    "step = 50\n",
    "end = 100\n",
    "\n",
    "# Create a range list\n",
    "range_list = list(range(start, end + step, step))\n",
    "\n",
    "# Create tuples for each consecutive pair in the list\n",
    "tuple_list = [(range_list[i], range_list[i+1]) for i in range(len(range_list)-1)]\n",
    "\n",
    "\n",
    "initialize = True \n",
    "input_file = 'data/GO_term_analysis/model_comparison_terms.csv'\n",
    "input_sep = constant.GO_FILE_SEP\n",
    "set_index = constant.GO_INDEX_COL  \n",
    "gene_column = constant.GO_GENE_COL \n",
    "gene_sep = ' '\n",
    "\n",
    "## create a param file \n",
    "configs = glob('./jsonFiles/model_comparison_*.json')\n",
    "params = []\n",
    "for start, end in tuple_list:\n",
    "    for conf_file in configs:\n",
    "        model_names = '_'.join(conf_file.split('/')[-1].split('.')[0].split('_')[1:])\n",
    "        print(model_names)\n",
    "        \n",
    "        out_file = f'data/GO_term_analysis/model_compare/LLM_processed_model_compare_{model_names}_{start}_{end}'  \n",
    "        param = f\"--config {conf_file} \\\n",
    "            --initialize \\\n",
    "            --input {input_file} \\\n",
    "            --input_sep  '{input_sep}'\\\n",
    "            --set_index {set_index} \\\n",
    "            --gene_column {gene_column}\\\n",
    "            --gene_sep '{gene_sep}' \\\n",
    "            --start {start} \\\n",
    "            --end {end} \\\n",
    "            --output_file {out_file}\"\n",
    "        print(param)\n",
    "        params.append(param)\n",
    "print('number of params: ', len(params))\n",
    "    \n",
    "\n",
    "with open('model_compare_params.txt', 'w') as f:\n",
    "    for p in params:\n",
    "        f.write(p+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--config ./jsonFiles/model_comparison_llama2_70b.json         --initialize  True        --input data/GO_term_analysis/model_comparison_terms.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 0         --end 50         --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_llama2_70b_0_50\n",
      "--config ./jsonFiles/model_comparison_llama2_70b.json         --initialize  True        --input data/GO_term_analysis/model_comparison_terms.csv         --input_sep  ','        --set_index GO         --gene_column Genes        --gene_sep ' '         --start 50         --end 100         --output_file data/GO_term_analysis/model_compare/LLM_processed_model_compare_llama2_70b_50_100\n",
      "number of params:  10\n"
     ]
    }
   ],
   "source": [
    "## add llama2 70b to this parameter file since we had the server issue fixed\n",
    "# open the param \n",
    "import os \n",
    "from glob import glob\n",
    "# Define start, step, and end values\n",
    "start = 0\n",
    "step = 50\n",
    "end = 100\n",
    "\n",
    "# Create a range list\n",
    "range_list = list(range(start, end + step, step))\n",
    "\n",
    "# Create tuples for each consecutive pair in the list\n",
    "tuple_list = [(range_list[i], range_list[i+1]) for i in range(len(range_list)-1)]\n",
    "\n",
    "\n",
    "initialize = True \n",
    "input_file = 'data/GO_term_analysis/model_comparison_terms.csv'\n",
    "input_sep = constant.GO_FILE_SEP\n",
    "set_index = constant.GO_INDEX_COL  \n",
    "gene_column = constant.GO_GENE_COL \n",
    "gene_sep = ' '\n",
    "conf_file = './jsonFiles/model_comparison_llama2_70b.json'\n",
    "\n",
    "with open('model_compare_params.txt', 'r') as f:\n",
    "    params = f.readlines()\n",
    "params = [p.strip() for p in params]\n",
    "\n",
    "model_names = 'llama2_70b'\n",
    "for start, end in tuple_list:\n",
    "    out_file = f'data/GO_term_analysis/model_compare/LLM_processed_model_compare_{model_names}_{start}_{end}'  \n",
    "    param = f\"--config {conf_file} \\\n",
    "        --initialize \\\n",
    "        --input {input_file} \\\n",
    "        --input_sep  '{input_sep}'\\\n",
    "        --set_index {set_index} \\\n",
    "        --gene_column {gene_column}\\\n",
    "        --gene_sep '{gene_sep}' \\\n",
    "        --start {start} \\\n",
    "        --end {end} \\\n",
    "        --output_file {out_file}\"\n",
    "    print(param)\n",
    "    params.append(param)\n",
    "print('number of params: ', len(params))\n",
    "\n",
    "# save the param file\n",
    "with open('model_compare_params.txt', 'w') as f:\n",
    "    for p in params:\n",
    "        f.write(p+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkout and combine the output from the batch run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini_pro_50_100 gemini_pro_default Analysis pass\n",
      "-----------------------\n",
      "gemini_pro_50_100 gemini_pro_50perc_contaminated Analysis pass\n",
      "-----------------------\n",
      "gemini_pro_50_100 gemini_pro_100perc_contaminated Analysis pass\n",
      "-----------------------\n",
      "mixtral_latest_0_50 mixtral_latest_default Analysis pass\n",
      "-----------------------\n",
      "mixtral_latest_0_50 mixtral_latest_50perc_contaminated Analysis has 1 None in the analysis column\n",
      "-----------------------\n",
      "mixtral_latest_0_50 mixtral_latest_100perc_contaminated Analysis has 1 None in the analysis column\n",
      "-----------------------\n",
      "mixtral_latest_50_100 mixtral_latest_default Analysis has 1 None in the analysis column\n",
      "-----------------------\n",
      "mixtral_latest_50_100 mixtral_latest_50perc_contaminated Analysis pass\n",
      "-----------------------\n",
      "mixtral_latest_50_100 mixtral_latest_100perc_contaminated Analysis pass\n",
      "-----------------------\n",
      "gpt_4_50_100 gpt_4_default Analysis pass\n",
      "-----------------------\n",
      "gpt_4_50_100 gpt_4_50perc_contaminated Analysis pass\n",
      "-----------------------\n",
      "gpt_4_50_100 gpt_4_100perc_contaminated Analysis pass\n",
      "-----------------------\n",
      "gemini_pro_0_50 gemini_pro_default Analysis pass\n",
      "-----------------------\n",
      "gemini_pro_0_50 gemini_pro_50perc_contaminated Analysis pass\n",
      "-----------------------\n",
      "gemini_pro_0_50 gemini_pro_100perc_contaminated Analysis pass\n",
      "-----------------------\n",
      "gpt_4_0_50 gpt_4_default Analysis pass\n",
      "-----------------------\n",
      "gpt_4_0_50 gpt_4_50perc_contaminated Analysis pass\n",
      "-----------------------\n",
      "gpt_4_0_50 gpt_4_100perc_contaminated Analysis pass\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "processed_files = glob('data/GO_term_analysis/model_compare/LLM_processed_model_compare*.tsv')\n",
    "# processed_files\n",
    "# check any with None in the analysis column\n",
    "for file in processed_files:\n",
    "    model_names = '_'.join(file.split('/')[-1].split('.')[0].split('_')[-4:])\n",
    "    \n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    # column names end with Analysis\n",
    "    analysis_cols = [col for col in df.columns if col.endswith('Analysis')]\n",
    "    for col in analysis_cols:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            n_none = df[col].isna().sum()\n",
    "            print(f'{model_names} {col} has {n_none} None in the analysis column')\n",
    "        else:\n",
    "            print(f'{model_names} {col} pass')\n",
    "        print('-----------------------')\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini_pro\n",
      "['data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gemini_pro_50_100.tsv', 'data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gemini_pro_0_50.tsv']\n",
      "(100, 14)\n",
      "------------saved--------------\n",
      "mixtral_latest\n",
      "['data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_mixtral_latest_0_50.tsv', 'data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_mixtral_latest_50_100.tsv']\n",
      "(100, 14)\n",
      "------------saved--------------\n",
      "llama2_70b\n",
      "['data/GO_term_analysis/model_compare/LLM_processed_model_compare_llama2_70b_50_100.tsv', 'data/GO_term_analysis/model_compare/LLM_processed_model_compare_llama2_70b_0_50.tsv']\n",
      "(100, 14)\n",
      "------------saved--------------\n",
      "gpt_4\n",
      "['data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gpt_4_50_100.tsv', 'data/GO_term_analysis/model_compare/LLM_processed_model_compare_comparison_gpt_4_0_50.tsv']\n",
      "(100, 14)\n",
      "------------saved--------------\n"
     ]
    }
   ],
   "source": [
    "## combine the 0-50 and 50-100 files together\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "processed_files = glob('data/GO_term_analysis/model_compare/LLM_processed_model_compare*.tsv')\n",
    "\n",
    "model_names = []\n",
    "for file in processed_files:\n",
    "    model_name = '_'.join(file.split('/')[-1].split('.')[0].split('_')[-4:-2])\n",
    "    model_names.append(model_name)\n",
    "model_names = list(set(model_names))\n",
    "for model in model_names:\n",
    "    print(model)\n",
    "    files = [file for file in processed_files if model in file]\n",
    "    print(files)\n",
    "    df = pd.concat([pd.read_csv(file, sep='\\t', index_col='GO') for file in files])\n",
    "    \n",
    "    # add the toy example in as well \n",
    "    toy_file = f'data/GO_term_analysis/LLM_processed_toy_example_w_contamination_{model}.tsv'\n",
    "    \n",
    "    df = pd.concat([df, pd.read_csv(toy_file, sep='\\t', index_col='GO')])\n",
    "    # check any with None in the analysis column\n",
    "    analysis_columns = [col for col in df.columns if col.endswith('Analysis')]\n",
    "    for col in analysis_columns:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            n_none = df[col].isna().sum()\n",
    "            print(f'{model} {col} has {n_none} None in the analysis column')\n",
    "    \n",
    "    print(df.shape)\n",
    "    df.to_csv(f'data/GO_term_analysis/model_compare/LLM_processed_model_compare_100set_{model}.tsv', sep='\\t', index=True)\n",
    "    print('------------saved--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     # print(ranges)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 29\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocessed_files\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAny duplicated GO: \u001b[39m\u001b[38;5;124m'\u001b[39m,combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGO\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum())\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_eval/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_eval/lib/python3.11/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_eval/lib/python3.11/site-packages/pandas/core/reshape/concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "### sanity check code along the way\n",
    "processed_files = glob('data/GO_term_analysis/LLM_processed_selected_go_terms*.tsv')\n",
    "\n",
    "for file in processed_files:\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    df.set_index('GO', inplace=True)\n",
    "    ranges = file.split('/')[-1].split('.')[0].split('_')[5:7]\n",
    "    with open(f'data/GO_term_analysis/LLM_response_go_terms_{ranges[0]}_{ranges[1]}.json') as fp:\n",
    "        llm_response_dict = json.load(fp)\n",
    "    for go_term, row in df.iterrows():\n",
    "        if llm_response_dict[go_term] == 'NO ANALYSIS':\n",
    "            print(file.split('/')[-1])\n",
    "            print(f'No analysis for {go_term}')\n",
    "            continue\n",
    "        else:\n",
    "            llm_analysis = llm_response_dict[go_term].split('\\n', 2)[2]\n",
    "            if df.loc[go_term, 'LLM Analysis'] != llm_analysis:\n",
    "                print(f'LLM analysis for {go_term} is different')\n",
    "            \n",
    "    df.reset_index(inplace=True)\n",
    "#     # print(ranges)\n",
    "    print(df.shape)\n",
    "\n",
    "    \n",
    "combined_df = pd.concat([pd.read_csv(f, sep = '\\t') for f in processed_files])\n",
    "print(combined_df.shape)\n",
    "print('Any duplicated GO: ',combined_df['GO'].duplicated().sum())\n",
    "print('Any NAs in the LLM res: ', combined_df['LLM Name'].isna().sum())\n",
    "print('Any duplicated LLM analysis: ', combined_df['LLM Analysis'].duplicated(keep=False).sum())\n",
    "\n",
    "combined_df.to_csv('data/GO_term_analysis/LLM_processed_selected_1000_go_terms.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
