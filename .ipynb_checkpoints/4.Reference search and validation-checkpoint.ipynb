{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for running literature search and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json \n",
    "from Bio import Entrez\n",
    "import openai\n",
    "from utils.reference_checker import get_references_for_paragraphs\n",
    "import pickle"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "runVersion = 'initial'#'initial'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "runOnlyExamples = False"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "dataType = \"Omics\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "if dataType == \"Omics\":\n",
    "    LLM_analysisFilePath  = 'data/omics_revamped_LLM_Enrichr_simVals_DF.tsv'\n",
    "    toSaveFilePath = 'data/omics_revamped_LLM_Enrichr_simVals_refs_2_DF.tsv'\n",
    "    nameCol = 'GeneSetName'\n",
    "    \n",
    "\n",
    "elif dataType == \"GO_sets\":\n",
    "    LLM_analysisFilePath = 'data/GO_term_analysis/simrank_LLM_processed_selected_1000_go_terms.tsv'\n",
    "    toSaveFilePath = 'data/GO_term_analysis/simrank_LLM_processed_selected_1000_go_terms_refs.tsv'\n",
    "    jsonFilePath = 'reference_checking_task1.json'\n",
    "    examplesTORun = [\"GO:0019433\"] \n",
    "    nameCol = 'GO'\n",
    "    \n",
    "else:\n",
    "    print(\"Not implemented for dataType\")\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "if runVersion == 'additional':\n",
    "    LLM_analysisFilePath = toSaveFilePath"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "LLM_analysisFilePath"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "jsonFilePath = 'jsonFiles/reference_checking_revision.json'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "with open(jsonFilePath) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "email = config['EMAIL']\n",
    "\n",
    " # save the paragraph with corresponding keywords and references to a json file \n",
    "savejsonPath = 'paragraph_ref_data_revision_240129' #this is the default, change to your own json file name (no need to add '.json') \n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"] # Environment variable"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "MarkedParagraphs = []"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read in the LLM analysis file\n",
    "df = pd.read_csv(LLM_analysisFilePath, sep='\\t', keep_default_na=False, na_values=['NaN'])\n",
    "df = df.replace({'None':None})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "source": [
    "if (runVersion == \"initial\") or (runVersion == \"rerun\") or (runVersion == \"test\"):\n",
    "    df['References'] = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "source": [
    "for i, row in df.iterrows():\n",
    "#for i in range(startRow, df.shape[0]):\n",
    "#    row = df.iloc[i]\n",
    "  #  if runOnlyExamples: # Only run examples\n",
    "  #      if df.iloc[i][nameCol] not in examplesTORun: \n",
    "  #          continue\n",
    "\n",
    "    \n",
    "    if runVersion == \"initial\":\n",
    "        if df.iloc[i]['References'] is not None:\n",
    "            continue\n",
    "    if runVersion == \"additional\":\n",
    "        if not (df.iloc[i]['References'] == ''):\n",
    "            continue # skip this row because already done\n",
    "    \n",
    "    print('=========================================')\n",
    "    print('=========================================')\n",
    "    print('=========================================')\n",
    "\n",
    "    print(['dataframe row', i])\n",
    "    # check out the llm analysis \n",
    "    example_analysis = df.iloc[i]['LLM Analysis']\n",
    "    paragraphs = list(filter(lambda p: len(p.split()) > 5, example_analysis.split(\"\\n\")))\n",
    "    \n",
    "    try:\n",
    "        references = get_references_for_paragraphs(paragraphs, email = email, config =config, n=3, verbose=True, MarkedParagraphs = MarkedParagraphs, saveto=savejsonPath)\n",
    "        print(['Cannot get references for row', i])\n",
    "    except:\n",
    "        references = ''\n",
    "        \n",
    "    references.replace( '\\n', '')\n",
    "        \n",
    "    df.loc[i, 'References'] = references\n",
    "    if i%5==0:\n",
    "        df.to_csv(toSaveFilePath, sep = '\\t')\n",
    "        \n",
    "if not runOnlyExamples:\n",
    "    if MarkedParagraphs:\n",
    "        with open('data/marked_paragraph_reference.json', 'w') as fp:\n",
    "            json.dump(MarkedParagraphs, fp)\n",
    "if not runOnlyExamples: \n",
    "    df.to_csv(toSaveFilePath, sep = '\\t')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# save MarkedParagraphs\n",
    "with open('data/MarkedParagraphs.pickle', 'wb') as f:\n",
    "    pickle.dump(MarkedParagraphs, f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df.to_csv(toSaveFilePath, sep = '\\t')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "llm_eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
