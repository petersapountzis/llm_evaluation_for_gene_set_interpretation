{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query GPT-4 for name and analysis using a toy example\n",
    "\n",
    "#### This uses an improved version of the original prompt that includes instructions to generate an LLM Confidence Score.\n",
    "\n",
    "#### The prompt also includes an example analysis to help the LLM in its task.\n",
    "\n",
    "#### The LLM Score has its own column in the output TSV file.\n",
    "\n",
    "#### The JSON config file is updated to use \"GPT-4_1106-preview\" build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "from utils.openai_query import openai_chat\n",
    "from utils.prompt_factory import make_user_prompt_with_score\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "## check example_config.json for the format of the config file\n",
    "with open('./jsonFiles/GOLLMrun_config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "context = config['CONTEXT']\n",
    "gpt_model = config['GPT_MODEL']\n",
    "temperature = config['TEMP']\n",
    "max_tokens = config['MAX_TOKENS']\n",
    "rate_per_token = config['RATE_PER_TOKEN']\n",
    "LOG_FILE = config['LOG_NAME'] + '_log.json'\n",
    "DOLLAR_LIMIT = config['DOLLAR_LIMIT']\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"] # set your openai api key in the environment variable or set in config\n",
    "# Generate list of genes from file (file: data/go_terms_sample.csv) check notebook 0.[Prep GO] Download_and_parse_GO.ipynb\n",
    "df = pd.read_csv('data/GO_term_analysis/toy_example.csv', sep = ',',index_col=0)\n",
    "\n",
    "df['LLM Name'] = None\n",
    "df['LLM Analysis'] = None\n",
    "df['LLM Score'] = None\n",
    "# print(df.head())\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    term_genes = row['Genes']\n",
    "    genes = term_genes.split()\n",
    "    prompt = make_user_prompt_with_score(genes)\n",
    "    # print(prompt)\n",
    "    analysis = openai_chat(context, prompt, gpt_model,temperature, max_tokens, rate_per_token, LOG_FILE, DOLLAR_LIMIT)\n",
    "    \n",
    "    llm_process = analysis.split(\"\\n\")[0].replace(\"Process: \", \"\")\n",
    "    \n",
    "    llm_score = llm_process.split(\" \")[-1].strip(\"()\")\n",
    "    df.loc[i, 'LLM Score'] = llm_score\n",
    "    \n",
    "    llm_name = llm_process.rsplit(\" \", 1)[0]\n",
    "    df.loc[i, 'LLM Name'] = llm_name\n",
    "    \n",
    "    llm_analysis = analysis.split('\\n', 2)[2]\n",
    "    df.loc[i, 'LLM Analysis'] = llm_analysis\n",
    "    # go_name = row['Term_Description'].lower()\n",
    "    # print(go_name)\n",
    "\n",
    "df.to_csv('data/GO_term_analysis/LLM_processed_toy_example.tsv', index=True, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the script for batch run\n",
    "\n",
    "input_file = 'data/GO_term_analysis/toy_example.csv'\n",
    "config = './jsonFiles/GOLLMrun_config.json'\n",
    "%run query_llm_for_analysis.py --input $input_file --start 0 --end 1 --config $config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkout and combine the output from the batch run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "### sanity check code along the way\n",
    "processed_files = glob('data/GO_term_analysis/LLM_processed_selected_go_terms*.tsv')\n",
    "\n",
    "for file in processed_files:\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    df.set_index('GO', inplace=True)\n",
    "    ranges = file.split('/')[-1].split('.')[0].split('_')[5:7]\n",
    "    with open(f'data/GO_term_analysis/LLM_response_go_terms_{ranges[0]}_{ranges[1]}.json') as fp:\n",
    "        llm_response_dict = json.load(fp)\n",
    "    for go_term, row in df.iterrows():\n",
    "        if llm_response_dict[go_term] == 'NO ANALYSIS':\n",
    "            print(file.split('/')[-1])\n",
    "            print(f'No analysis for {go_term}')\n",
    "            continue\n",
    "        else:\n",
    "            llm_analysis = llm_response_dict[go_term].split('\\n', 2)[2]\n",
    "            if df.loc[go_term, 'LLM Analysis'] != llm_analysis:\n",
    "                print(f'LLM analysis for {go_term} is different')\n",
    "            \n",
    "    df.reset_index(inplace=True)\n",
    "#     # print(ranges)\n",
    "    print(df.shape)\n",
    "\n",
    "    \n",
    "combined_df = pd.concat([pd.read_csv(f, sep = '\\t') for f in processed_files])\n",
    "print(combined_df.shape)\n",
    "print('Any duplicated GO: ',combined_df['GO'].duplicated().sum())\n",
    "print('Any NAs in the LLM res: ', combined_df['LLM Name'].isna().sum())\n",
    "print('Any duplicated LLM analysis: ', combined_df['LLM Analysis'].duplicated(keep=False).sum())\n",
    "\n",
    "combined_df.to_csv('data/GO_term_analysis/LLM_processed_selected_1000_go_terms.tsv', index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
