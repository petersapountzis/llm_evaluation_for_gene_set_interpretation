{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook queries GPT-4 and formats its response to obtain a succint name for each of the 'omics gene sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json \n",
    "import openai"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import math"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "from utils.openai_query import openai_chat\n",
    "from utils.prompt_factory import make_user_prompt_with_score\n",
    "from utils.llm_analysis_utils import process_analysis, save_progress"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"] # Environment variable"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "runVersion = \"additional\"; # initial"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "geneSep = \" \"\n",
    "inputFilePath = \"data/omics_revamped_LLM_DF.tsv\"; #\"data/omics_revamped.txt\"\n",
    "jsonFilePath = \"jsonFiles/OmicsRunLLM.json\"\n",
    "genesCol = \"GeneList\"\n",
    "nameCol  = \"GeneSetName\"\n",
    "outputFilePath = \"data/omics_revamped_LLM_DF.tsv\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "with open(jsonFilePath) as json_file:\n",
    "    config = json.load(json_file)\n",
    "    \n",
    "context = config['CONTEXT']\n",
    "gpt_model = config['GPT_MODEL']\n",
    "temperature = config['TEMP']\n",
    "max_tokens = config['MAX_TOKENS']\n",
    "rate_per_token = config['RATE_PER_TOKEN']\n",
    "LOG_FILE = config['LOG_NAME'] + '240129'+'log.json'\n",
    "DOLLAR_LIMIT = config['DOLLAR_LIMIT']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "SEED = 42"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "gpt_model"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GPT-4 query pipeline for NeST gene sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "source": [
    "df = pd.read_csv(inputFilePath, sep = \"\\t\"); "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "if runVersion == \"initial\":\n",
    "    df['LLM Name'] = None\n",
    "    df['LLM Analysis'] = None\n",
    "    df['Score'] = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "for i, row in df.iterrows():\n",
    "    \n",
    "    term_genes = row[genesCol]\n",
    "    genes = term_genes.split(geneSep) \n",
    "    \n",
    "    if runVersion == \"additional\":\n",
    "        if type(row['LLM Name']) == str:\n",
    "            continue # skip this row because already done\n",
    "   \n",
    "    \n",
    "    prompt = make_user_prompt_with_score(genes)\n",
    "\n",
    "    analysis, finger_print = openai_chat(context, prompt, gpt_model, temperature, max_tokens, rate_per_token, LOG_FILE, DOLLAR_LIMIT, SEED)\n",
    "\n",
    "    if analysis:\n",
    "        llm_name, llm_score, llm_analysis = process_analysis(analysis)\n",
    "        df.loc[i, 'LLM Name'] = llm_name\n",
    "        df.loc[i, 'LLM Analysis'] = llm_analysis\n",
    "        df.loc[i, 'Score'] = float(llm_score)\n",
    "\n",
    "    else:\n",
    "        #go_term = row['GO']\n",
    "        name = row[nameCol]\n",
    "        print(f'No analysis for {name}')\n",
    "        df.loc[i, 'LLM Name'] = None\n",
    "        df.loc[i, 'LLM Analysis'] = None\n",
    "    #if (i%10 ==1):\n",
    "    #    break\n",
    "        \n",
    "    # Keep on saving to not loose data if something happens\n",
    "    if (i%10 == 1):\n",
    "        print(i)\n",
    "        df.to_csv(outputFilePath, sep = \"\\t\",  index=False)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "df.to_csv(outputFilePath, sep= '\\t', index=False)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "llm_eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
