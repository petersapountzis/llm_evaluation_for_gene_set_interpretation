{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:38:33.239153Z",
     "start_time": "2025-02-27T14:38:28.329005Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "from utils.openai_query import openai_chat\n",
    "from utils.prompt_factory import make_user_prompt_with_score,  prompt_for_name\n",
    "from utils.server_model_query import server_model_chat\n",
    "from utils.llm_analysis_utils import process_analysis, save_progress\n",
    "from utils.genai_query import query_genai_model\n",
    "from tqdm import tqdm\n",
    "import constant\n",
    "import openai\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai QUERY being run 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\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:38:33.257258Z",
     "start_time": "2025-02-27T14:38:33.241001Z"
    }
   },
   "source": [
    "## load variables\n",
    "initialize = True # if True, then initialize the input table with llm names, analysis and score to None \n",
    "# Replace with your actual values\n",
    "config_file = './jsonFiles/name_only_test_gpt_4.json'  # replace with your actual config file \n",
    "input_file = 'data/GO_term_analysis/100_selected_go_contaminated.csv' # replace with your actual input file\n",
    "input_sep = ','  # replace with the separator\n",
    "set_index = 'GO'  # replace with your column name that you want to set as index or None\n",
    "gene_column = 'Genes'  # replace with your actual column name for the gene list\n",
    "gene_sep = ' '  # replace with your actual separator\n",
    "gene_features = None  # replace with your path to the gene features or None if you don't want to include in the prompt\n",
    "direct = False # if True, then the prompt will be a direct sentence asking for a name and analysis from the gene set, otherwise default or customized prompt\n",
    "out_file = 'data/GO_term_analysis/name_only/LLM_processed_only_name_gpt4'  # replace with your actual output file name\n",
    "\n",
    "\n",
    "# load the config file\n",
    "with open(config_file) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "# Load OpenAI key, context, and model used \n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "context = config['CONTEXT']\n",
    "model = config['MODEL']\n",
    "temperature = config['TEMP']\n",
    "max_tokens = config['MAX_TOKENS']\n",
    "if model.startswith('gpt'):\n",
    "    rate_per_token = config['RATE_PER_TOKEN']\n",
    "    DOLLAR_LIMIT = config['DOLLAR_LIMIT']\n",
    "LOG_FILE = config['LOG_NAME']+'_log.json'\n",
    "\n",
    "SEED = constant.SEED\n",
    "column_prefix = model.split('-')[0]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:38:35.862881Z",
     "start_time": "2025-02-27T14:38:34.030131Z"
    }
   },
   "source": [
    "df = pd.read_csv(input_file, sep=input_sep, index_col=set_index)\n",
    "\n",
    "genes = df.loc['GO:0061740', 'Genes'].split(' ')\n",
    "print(prompt_for_name(genes))\n",
    "prompt_test = prompt_for_name(genes)\n",
    "openai_chat(context, prompt_test, model, temperature, max_tokens, rate_per_token, LOG_FILE, DOLLAR_LIMIT, SEED)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propose a brief name for the most prominant biological process performed by the system.\n",
      "    \n",
      "Be concise, do not use unneccesary words. Be specific, avoid overly general names such as 'the proteins are involved in various cellular processes'\n",
      "Be factual, do not editorialize.\n",
      "    \n",
      "\n",
      "Here are the interacting proteins:\n",
      "\n",
      "Proteins: HSPA8, LAMP2, CLU.\n",
      "\n",
      "\n",
      "117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Chaperone-Mediated Autophagy (CMA)', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:38:42.256769Z",
     "start_time": "2025-02-27T14:38:42.234272Z"
    }
   },
   "source": [
    "# handle the logger so it create a new one for each model run\n",
    "def get_logger(filename):\n",
    "    logger = logging.getLogger(filename)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(filename)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def main(df):\n",
    "    analysis_dict  = {}\n",
    "\n",
    "    logger = get_logger(f'{out_file}.log')\n",
    "\n",
    "    i = 0 #used for track progress and saving the file\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        #only process None rows \n",
    "        if pd.notna(row[f'{column_prefix} Only Name']):\n",
    "            continue\n",
    "        \n",
    "        gene_data = row[gene_column]\n",
    "        # if gene_data is not a string, then skip\n",
    "        if type(gene_data) != str:\n",
    "            \n",
    "            logger.warning(f'Gene set {idx} is not a string, skipping')\n",
    "            continue\n",
    "        genes = gene_data.split(gene_sep)\n",
    "        \n",
    "        if len(genes) >1000:\n",
    "            logger.warning(f'Gene set {idx} is too big, skipping')\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            prompt = prompt_for_name(genes)\n",
    "            # print(prompt)\n",
    "            finger_print = None\n",
    "            if model.startswith('gpt'):\n",
    "                print(\"Accessing OpenAI API\")\n",
    "                analysis, finger_print = openai_chat(context, prompt, model, temperature, max_tokens, rate_per_token, LOG_FILE, DOLLAR_LIMIT, SEED)\n",
    "            elif model.startswith('gemini'):\n",
    "                print(\"Using Google Gemini API\")\n",
    "                analysis, error_message = query_genai_model(f\"{context}\\n{prompt}\", model, temperature, max_tokens, LOG_FILE) \n",
    "            else:\n",
    "                print(\"Using server model\")\n",
    "                analysis, error_message= server_model_chat(context, prompt, model, temperature, max_tokens,LOG_FILE, SEED)\n",
    "\n",
    "            \n",
    "            if analysis:\n",
    "                # print(analysis)\n",
    "                df.loc[idx, f'{column_prefix} Only Name'] = analysis\n",
    "                analysis_dict[f'{idx}_{column_prefix}'] = analysis\n",
    "                # Log success with fingerprint\n",
    "                logger.info(f'Success for {idx} {column_prefix}.')\n",
    "                if finger_print:\n",
    "                    logger.info(f'GPT_Fingerprint for {idx}: {finger_print}')\n",
    "                    \n",
    "            else:\n",
    "                logger.error(f'Error for query gene set {idx}: {error_message}')\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error for {idx}: {e}')\n",
    "            continue\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            save_progress(df, analysis_dict, out_file)\n",
    "            # df.to_csv(f'{out_file}.tsv', sep='\\t', index=True)\n",
    "            print(f\"Saved progress for {i} genesets\")\n",
    "    # save the final file\n",
    "    save_progress(df, analysis_dict, out_file)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:38:43.343846Z",
     "start_time": "2025-02-27T14:38:43.176953Z"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df = pd.read_csv(input_file, sep=input_sep, index_col=set_index)\n",
    "    column_prefix = 'gpt4_default'\n",
    "    print(column_prefix)\n",
    "    \n",
    "    if initialize:\n",
    "        df[f'{column_prefix} Only Name'] = None\n",
    "    main(df)  ## run with the real set \n",
    "    \n",
    "    df.head()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4_default\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/petersapountzis/Desktop/tulaneCBG/EnrichGPT/llm_evaluation_for_gene_set_interpretation/data/GO_term_analysis/name_only/LLM_processed_only_name_gpt4.log'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m initialize:\n\u001B[1;32m      8\u001B[0m     df[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumn_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Only Name\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m## run with the real set \u001B[39;00m\n\u001B[1;32m     11\u001B[0m df\u001B[38;5;241m.\u001B[39mhead()\n",
      "Cell \u001B[0;32mIn[4], line 16\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mmain\u001B[39m(df):\n\u001B[1;32m     14\u001B[0m     analysis_dict  \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m---> 16\u001B[0m     logger \u001B[38;5;241m=\u001B[39m \u001B[43mget_logger\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mout_file\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.log\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     i \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;66;03m#used for track progress and saving the file\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m idx, row \u001B[38;5;129;01min\u001B[39;00m tqdm(df\u001B[38;5;241m.\u001B[39miterrows(), total\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;66;03m#only process None rows \u001B[39;00m\n",
      "Cell \u001B[0;32mIn[4], line 6\u001B[0m, in \u001B[0;36mget_logger\u001B[0;34m(filename)\u001B[0m\n\u001B[1;32m      4\u001B[0m logger\u001B[38;5;241m.\u001B[39msetLevel(logging\u001B[38;5;241m.\u001B[39mINFO)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m logger\u001B[38;5;241m.\u001B[39mhandlers:\n\u001B[0;32m----> 6\u001B[0m     file_handler \u001B[38;5;241m=\u001B[39m \u001B[43mlogging\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFileHandler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     formatter \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mFormatter(\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%(asctime)s\u001B[39;00m\u001B[38;5;124m - \u001B[39m\u001B[38;5;132;01m%(levelname)s\u001B[39;00m\u001B[38;5;124m - \u001B[39m\u001B[38;5;132;01m%(message)s\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      8\u001B[0m     file_handler\u001B[38;5;241m.\u001B[39msetFormatter(formatter)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/llm_eval/lib/python3.11/logging/__init__.py:1181\u001B[0m, in \u001B[0;36mFileHandler.__init__\u001B[0;34m(self, filename, mode, encoding, delay, errors)\u001B[0m\n\u001B[1;32m   1179\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1181\u001B[0m     StreamHandler\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/llm_eval/lib/python3.11/logging/__init__.py:1213\u001B[0m, in \u001B[0;36mFileHandler._open\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1208\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1209\u001B[0m \u001B[38;5;124;03mOpen the current base file with the (original) mode and encoding.\u001B[39;00m\n\u001B[1;32m   1210\u001B[0m \u001B[38;5;124;03mReturn the resulting stream.\u001B[39;00m\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1212\u001B[0m open_func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_builtin_open\n\u001B[0;32m-> 1213\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopen_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbaseFilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1214\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/petersapountzis/Desktop/tulaneCBG/EnrichGPT/llm_evaluation_for_gene_set_interpretation/data/GO_term_analysis/name_only/LLM_processed_only_name_gpt4.log'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "# check the time \n",
    "import json \n",
    "with open('logs/name_only_gpt_4_log.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "time_use = data['time_taken_total']\n",
    "time_per_query = time_use/data['runs']\n",
    "cost = data['dollars_spent']\n",
    "dollor_per_query = cost/data['runs']\n",
    "print(f'Time per query: {time_per_query} seconds')\n",
    "print(f'Dollor per query: {dollor_per_query} dollars')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "## run semantic similarities on GO names vs the llm name\n",
    "from semanticSimFunctions import getSentenceEmbedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "SapBERT_tokenizer = AutoTokenizer.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n",
    "SapBERT_model = AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n",
    "\n",
    "nameOnly_df = pd.read_csv('data/GO_term_analysis/name_only/LLM_processed_only_name_gpt4.tsv', sep='\\t', index_col='GO')\n",
    "with open('./data/all_go_terms_3to100_embeddings_dict.pkl', 'rb') as handle:\n",
    "    all_go_terms_embeddings_dict = pickle.load(handle)\n",
    " \n",
    "\n",
    "df = nameOnly_df.copy()\n",
    "df['LLM_name_GO_term_sim'] = None\n",
    "for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "\n",
    "    GO_term = row['Term_Description'] # the actual GO term\n",
    "    # get the name column \n",
    "    name_col = [col for col in df.columns if 'name' in col.lower()][0]\n",
    "    # print(name_col)\n",
    "    LLM_name = row[name_col] # the LLM name\n",
    "    # print(LLM_name)\n",
    "    # get llm name embedding\n",
    "    LLM_name_emb = getSentenceEmbedding(LLM_name, SapBERT_tokenizer, SapBERT_model)\n",
    "    GO_emb = all_go_terms_embeddings_dict[GO_term]\n",
    "    # calculate the cosine similarity\n",
    "    sim = cosine_similarity(LLM_name_emb, GO_emb)[0][0]\n",
    "    df.loc[ind, 'LLM_name_GO_term_sim'] = sim\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# compare llm_go sim with the default pipeline \n",
    "default_df = pd.read_csv('./data/GO_term_analysis/model_compare/sim_rank_LLM_processed_model_compare_100set_gpt_4.tsv', sep='\\t', index_col='GO')\n",
    "\n",
    "comparison_df = pd.merge(df[['LLM_name_GO_term_sim']], default_df[['LLM_name_GO_term_sim']], left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# Rename columns for clarity\n",
    "comparison_df.columns = ['LLM_name_GO_term_sim_nameOnly', 'LLM_name_GO_term_sim_namewithAnalysis']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.violinplot(data=comparison_df, ax=ax, inner='quartile', cut=0)\n",
    "ax.set_xticklabels(['Only Name', 'Name with Analysis'], rotation=45)\n",
    "ax.set_ylabel('Semantic Similarity with GO Term Description')\n",
    "sns.despine()\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "combined_df = pd.merge(df, default_df[['gpt_4_default Name', 'LLM_name_GO_term_sim']], left_index=True, right_index=True, how='inner')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "for ind, row in tqdm(combined_df.iterrows(), total=combined_df.shape[0]):\n",
    "    name_only = row['gpt4_default Only Name']\n",
    "    name_w_ana = row['gpt_4_default Name']\n",
    "    name_only_emb = getSentenceEmbedding(name_only, SapBERT_tokenizer, SapBERT_model)\n",
    "    name_w_ana_emb = getSentenceEmbedding(name_w_ana, SapBERT_tokenizer, SapBERT_model)\n",
    "    sim = cosine_similarity(name_only_emb, name_w_ana_emb)[0][0]\n",
    "    combined_df.loc[ind, 'Name_only_vs_Name_w_ana_sim'] = sim\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.violinplot(data=combined_df, y='Name_only_vs_Name_w_ana_sim', ax=ax, inner='quartile', cut=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
